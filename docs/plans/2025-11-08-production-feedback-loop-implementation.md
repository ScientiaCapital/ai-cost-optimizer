# Production Deployment with Feedback Loop Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Deploy AI Cost Optimizer locally with PostgreSQL and automated learning pipeline that retrains routing weekly based on user quality ratings

**Architecture:** Three-layer system with FastAPI + PostgreSQL for production API, FeedbackTrainer background service for weekly retraining, Docker Compose orchestration. Confidence-based learning only updates routing when sufficient feedback samples collected.

**Tech Stack:** FastAPI, PostgreSQL, Docker, Docker Compose, Alembic, psycopg2, APScheduler

**Estimated Time:** 10-12 days (8 tasks, 1-2 days each)

---

## Task 1: PostgreSQL Migration Setup

**Goal:** Set up Alembic migrations and create new feedback tables in PostgreSQL schema

**Files:**
- Create: `alembic/versions/003_add_feedback_tables.py`
- Create: `app/database/postgres.py`
- Test: `tests/test_postgres_migration.py`
- Modify: `requirements.txt`

### Step 1: Add PostgreSQL dependencies

**File:** `requirements.txt`

Add:
```
psycopg2-binary>=2.9.9
alembic>=1.12.0
```

### Step 2: Install dependencies

```bash
pip install -r requirements.txt
```

### Step 3: Write migration test

**Create:** `tests/test_postgres_migration.py`

```python
"""Tests for PostgreSQL migration to feedback tables."""
import pytest
import psycopg2
from alembic import command
from alembic.config import Config


@pytest.fixture
def alembic_config():
    """Create Alembic config for testing."""
    config = Config("alembic.ini")
    config.set_main_option("sqlalchemy.url", "postgresql://test:test@localhost:5432/test_optimizer")
    return config


def test_feedback_tables_migration(alembic_config):
    """Test migration creates response_feedback table."""
    # Run migration
    command.upgrade(alembic_config, "head")

    # Connect and verify table exists
    conn = psycopg2.connect("postgresql://test:test@localhost:5432/test_optimizer")
    cursor = conn.cursor()

    cursor.execute("""
        SELECT column_name, data_type
        FROM information_schema.columns
        WHERE table_name = 'response_feedback'
        ORDER BY ordinal_position
    """)

    columns = cursor.fetchall()
    column_names = [col[0] for col in columns]

    assert 'id' in column_names
    assert 'request_id' in column_names
    assert 'quality_score' in column_names
    assert 'is_correct' in column_names
    assert 'prompt_pattern' in column_names

    cursor.close()
    conn.close()


def test_model_performance_history_migration(alembic_config):
    """Test migration creates model_performance_history table."""
    command.upgrade(alembic_config, "head")

    conn = psycopg2.connect("postgresql://test:test@localhost:5432/test_optimizer")
    cursor = conn.cursor()

    cursor.execute("""
        SELECT column_name
        FROM information_schema.columns
        WHERE table_name = 'model_performance_history'
    """)

    columns = [row[0] for row in cursor.fetchall()]

    assert 'pattern' in columns
    assert 'provider' in columns
    assert 'model' in columns
    assert 'avg_quality_score' in columns
    assert 'correctness_rate' in columns
    assert 'sample_count' in columns
    assert 'confidence_level' in columns

    cursor.close()
    conn.close()
```

### Step 4: Run test to verify it fails

```bash
pytest tests/test_postgres_migration.py -v
```

**Expected:** FAIL with "ModuleNotFoundError" or table not found

### Step 5: Create migration file

**Create:** `alembic/versions/003_add_feedback_tables.py`

```python
"""Add feedback tables for learning pipeline

Revision ID: 003_feedback_tables
Revises: 002_routing_metrics
Create Date: 2025-11-08
"""
from alembic import op
import sqlalchemy as sa


# revision identifiers
revision = '003_feedback_tables'
down_revision = '002_routing_metrics'  # Update to your latest migration
branch_labels = None
depends_on = None


def upgrade():
    """Create feedback and model performance tables."""

    # Create response_feedback table
    op.create_table(
        'response_feedback',
        sa.Column('id', sa.Integer(), primary_key=True),
        sa.Column('request_id', sa.Text(), nullable=False),
        sa.Column('timestamp', sa.DateTime(), nullable=False),

        # User ratings
        sa.Column('quality_score', sa.Integer(), nullable=False),
        sa.Column('is_correct', sa.Boolean(), nullable=True),
        sa.Column('is_helpful', sa.Boolean(), nullable=True),

        # Context for learning
        sa.Column('prompt_pattern', sa.Text(), nullable=True),
        sa.Column('selected_provider', sa.Text(), nullable=True),
        sa.Column('selected_model', sa.Text(), nullable=True),
        sa.Column('complexity_score', sa.Float(), nullable=True),

        # Metadata
        sa.Column('user_id', sa.Text(), nullable=True),
        sa.Column('session_id', sa.Text(), nullable=True),
        sa.Column('comment', sa.Text(), nullable=True),

        sa.ForeignKeyConstraint(['request_id'], ['routing_metrics.request_id'])
    )

    # Create indexes
    op.create_index('idx_feedback_pattern', 'response_feedback', ['prompt_pattern'])
    op.create_index('idx_feedback_model', 'response_feedback', ['selected_model'])
    op.create_index('idx_feedback_timestamp', 'response_feedback', ['timestamp'])

    # Create model_performance_history table
    op.create_table(
        'model_performance_history',
        sa.Column('id', sa.Integer(), primary_key=True),
        sa.Column('pattern', sa.Text(), nullable=False),
        sa.Column('provider', sa.Text(), nullable=False),
        sa.Column('model', sa.Text(), nullable=False),

        # Computed metrics
        sa.Column('avg_quality_score', sa.Float(), nullable=True),
        sa.Column('correctness_rate', sa.Float(), nullable=True),
        sa.Column('sample_count', sa.Integer(), nullable=True),
        sa.Column('confidence_level', sa.Text(), nullable=True),

        # Cost tracking
        sa.Column('avg_cost', sa.Float(), nullable=True),
        sa.Column('total_cost', sa.Float(), nullable=True),

        # Metadata
        sa.Column('updated_at', sa.DateTime(), nullable=False),
        sa.Column('retraining_run_id', sa.Text(), nullable=True),

        sa.UniqueConstraint('pattern', 'provider', 'model', 'retraining_run_id',
                          name='uq_model_performance')
    )


def downgrade():
    """Drop feedback tables."""
    op.drop_table('model_performance_history')
    op.drop_index('idx_feedback_timestamp', 'response_feedback')
    op.drop_index('idx_feedback_model', 'response_feedback')
    op.drop_index('idx_feedback_pattern', 'response_feedback')
    op.drop_table('response_feedback')
```

### Step 6: Create PostgreSQL connection helper

**Create:** `app/database/postgres.py`

```python
"""PostgreSQL database connection utilities."""
import os
import psycopg2
from psycopg2.extras import RealDictCursor
from contextlib import contextmanager


def get_database_url():
    """Get PostgreSQL database URL from environment."""
    return os.getenv(
        'DATABASE_URL',
        'postgresql://optimizer_user:password@localhost:5432/optimizer'
    )


@contextmanager
def get_connection():
    """Get database connection as context manager.

    Usage:
        with get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT ...")
    """
    conn = psycopg2.connect(get_database_url())
    try:
        yield conn
        conn.commit()
    except Exception:
        conn.rollback()
        raise
    finally:
        conn.close()


def get_cursor(conn, dict_cursor=True):
    """Get cursor from connection.

    Args:
        conn: Database connection
        dict_cursor: If True, return RealDictCursor (rows as dicts)

    Returns:
        Database cursor
    """
    if dict_cursor:
        return conn.cursor(cursor_factory=RealDictCursor)
    return conn.cursor()
```

### Step 7: Run test to verify it passes

```bash
# Note: Requires PostgreSQL running locally for tests
# docker run --name test-postgres -e POSTGRES_PASSWORD=test -p 5432:5432 -d postgres:15-alpine

pytest tests/test_postgres_migration.py -v
```

**Expected:** PASS (2 tests)

### Step 8: Commit

```bash
git add requirements.txt alembic/versions/003_add_feedback_tables.py app/database/postgres.py tests/test_postgres_migration.py
git commit -m "feat: add PostgreSQL feedback tables migration

- Create response_feedback table for quality ratings
- Create model_performance_history table for learned metrics
- Add PostgreSQL connection utilities
- Alembic migration with upgrade/downgrade

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 2: Feedback API Endpoint

**Goal:** Create `/feedback` endpoint to collect user quality ratings

**Files:**
- Create: `app/models/feedback.py`
- Modify: `app/main.py`
- Create: `app/database/feedback_store.py`
- Test: `tests/test_feedback_endpoint.py`

### Step 1: Write test for feedback endpoint

**Create:** `tests/test_feedback_endpoint.py`

```python
"""Tests for feedback API endpoint."""
import pytest
from fastapi.testclient import TestClient
from app.main import app


client = TestClient(app)


def test_submit_feedback_success():
    """Test submitting feedback returns success."""
    response = client.post("/feedback", json={
        "request_id": "test_request_123",
        "quality_score": 4,
        "is_correct": True,
        "comment": "Good answer"
    })

    assert response.status_code == 200
    data = response.json()
    assert data["status"] == "recorded"
    assert "feedback_id" in data
    assert data["message"] == "Thank you for feedback"


def test_submit_feedback_invalid_score():
    """Test invalid quality score returns 422."""
    response = client.post("/feedback", json={
        "request_id": "test_request",
        "quality_score": 6,  # Invalid: must be 1-5
        "is_correct": True
    })

    assert response.status_code == 422


def test_submit_feedback_missing_required():
    """Test missing required fields returns 422."""
    response = client.post("/feedback", json={
        "request_id": "test_request"
        # Missing quality_score and is_correct
    })

    assert response.status_code == 422


def test_submit_feedback_stores_in_database(tmp_path):
    """Test feedback is actually stored in database."""
    # Submit feedback
    response = client.post("/feedback", json={
        "request_id": "test_store_123",
        "quality_score": 5,
        "is_correct": True
    })

    feedback_id = response.json()["feedback_id"]

    # Verify in database
    from app.database.feedback_store import FeedbackStore
    store = FeedbackStore()
    feedback = store.get_by_id(feedback_id)

    assert feedback is not None
    assert feedback["request_id"] == "test_store_123"
    assert feedback["quality_score"] == 5
    assert feedback["is_correct"] is True
```

### Step 2: Run test to verify it fails

```bash
pytest tests/test_feedback_endpoint.py -v
```

**Expected:** FAIL with "404 Not Found" or import errors

### Step 3: Create feedback Pydantic models

**Create:** `app/models/feedback.py`

```python
"""Pydantic models for feedback."""
from pydantic import BaseModel, Field
from typing import Optional
from datetime import datetime


class FeedbackRequest(BaseModel):
    """Request model for submitting feedback."""

    request_id: str = Field(..., description="Request ID from routing decision")
    quality_score: int = Field(..., ge=1, le=5, description="Quality rating 1-5")
    is_correct: bool = Field(..., description="Was response factually correct")
    is_helpful: Optional[bool] = Field(None, description="Was response helpful")
    comment: Optional[str] = Field(None, max_length=1000, description="Optional comment")


class FeedbackResponse(BaseModel):
    """Response model for feedback submission."""

    status: str = Field(..., description="Status of submission")
    feedback_id: int = Field(..., description="ID of stored feedback")
    message: str = Field(..., description="Human-readable message")
```

### Step 4: Create feedback storage class

**Create:** `app/database/feedback_store.py`

```python
"""Database operations for feedback storage."""
import logging
from datetime import datetime
from typing import Optional, Dict, Any
from app.database.postgres import get_connection, get_cursor

logger = logging.getLogger(__name__)


class FeedbackStore:
    """Stores and retrieves feedback data."""

    def store_feedback(
        self,
        request_id: str,
        quality_score: int,
        is_correct: bool,
        is_helpful: Optional[bool] = None,
        comment: Optional[str] = None,
        user_id: Optional[str] = None,
        session_id: Optional[str] = None
    ) -> int:
        """Store feedback in database.

        Args:
            request_id: Request ID from routing decision
            quality_score: Quality rating 1-5
            is_correct: Was response correct
            is_helpful: Was response helpful
            comment: Optional user comment
            user_id: Optional user identifier
            session_id: Optional session identifier

        Returns:
            Feedback ID
        """
        with get_connection() as conn:
            cursor = get_cursor(conn, dict_cursor=False)

            # Get context from routing_metrics
            cursor.execute("""
                SELECT
                    selected_provider,
                    selected_model,
                    pattern_detected,
                    complexity_score
                FROM routing_metrics
                WHERE request_id = %s
            """, (request_id,))

            context = cursor.fetchone()

            if context:
                provider, model, pattern, complexity = context
            else:
                provider = model = pattern = None
                complexity = None

            # Insert feedback
            cursor.execute("""
                INSERT INTO response_feedback (
                    request_id, timestamp, quality_score, is_correct, is_helpful,
                    prompt_pattern, selected_provider, selected_model,
                    complexity_score, user_id, session_id, comment
                ) VALUES (
                    %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
                ) RETURNING id
            """, (
                request_id,
                datetime.now(),
                quality_score,
                is_correct,
                is_helpful,
                pattern,
                provider,
                model,
                complexity,
                user_id,
                session_id,
                comment
            ))

            feedback_id = cursor.fetchone()[0]

            logger.info(f"Stored feedback {feedback_id} for request {request_id}")

            return feedback_id

    def get_by_id(self, feedback_id: int) -> Optional[Dict[str, Any]]:
        """Get feedback by ID.

        Args:
            feedback_id: Feedback ID

        Returns:
            Feedback dict or None
        """
        with get_connection() as conn:
            cursor = get_cursor(conn, dict_cursor=True)

            cursor.execute("""
                SELECT * FROM response_feedback WHERE id = %s
            """, (feedback_id,))

            return cursor.fetchone()
```

### Step 5: Add endpoint to FastAPI

**Modify:** `app/main.py`

Add import:
```python
from app.models.feedback import FeedbackRequest, FeedbackResponse
from app.database.feedback_store import FeedbackStore
```

Add endpoint before the `if __name__ == "__main__"` block:
```python
# Initialize feedback store
feedback_store = FeedbackStore()


@app.post("/feedback", response_model=FeedbackResponse)
async def submit_feedback(request: FeedbackRequest):
    """Submit quality feedback for a request.

    Args:
        request: Feedback submission

    Returns:
        Feedback confirmation
    """
    try:
        feedback_id = feedback_store.store_feedback(
            request_id=request.request_id,
            quality_score=request.quality_score,
            is_correct=request.is_correct,
            is_helpful=request.is_helpful,
            comment=request.comment
        )

        return FeedbackResponse(
            status="recorded",
            feedback_id=feedback_id,
            message="Thank you for feedback"
        )

    except Exception as e:
        logger.error(f"Failed to store feedback: {e}")
        raise HTTPException(
            status_code=500,
            detail="Failed to store feedback"
        )
```

### Step 6: Run test to verify it passes

```bash
pytest tests/test_feedback_endpoint.py -v
```

**Expected:** PASS (4 tests)

### Step 7: Commit

```bash
git add app/models/feedback.py app/database/feedback_store.py app/main.py tests/test_feedback_endpoint.py
git commit -m "feat: add /feedback endpoint for quality ratings

- FeedbackRequest/Response Pydantic models
- FeedbackStore for database operations
- POST /feedback endpoint with validation
- Links feedback to routing_metrics context
- Full test coverage

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 3: FeedbackTrainer Learning Pipeline

**Goal:** Create automated retraining system that updates routing from feedback

**Files:**
- Create: `app/learning/feedback_trainer.py`
- Test: `tests/test_feedback_trainer.py`

### Step 1: Write test for confidence thresholds

**Create:** `tests/test_feedback_trainer.py`

```python
"""Tests for FeedbackTrainer learning pipeline."""
import pytest
from datetime import datetime, timedelta
from app.learning.feedback_trainer import FeedbackTrainer


@pytest.fixture
def trainer(tmp_path):
    """Create FeedbackTrainer with test database."""
    # Setup test database connection
    return FeedbackTrainer(db_url="postgresql://test:test@localhost:5432/test_optimizer")


def test_confidence_calculation_high(trainer):
    """Test high confidence requires 10+ samples with good quality."""
    confidence = trainer._calculate_confidence(
        sample_count=12,
        avg_quality=4.2,
        correctness_rate=0.85
    )

    assert confidence == "high"


def test_confidence_calculation_medium(trainer):
    """Test medium confidence requires 5+ samples."""
    confidence = trainer._calculate_confidence(
        sample_count=7,
        avg_quality=3.8,
        correctness_rate=0.75
    )

    assert confidence == "medium"


def test_confidence_calculation_low(trainer):
    """Test low confidence for insufficient samples."""
    confidence = trainer._calculate_confidence(
        sample_count=3,
        avg_quality=4.5,
        correctness_rate=0.9
    )

    assert confidence == "low"


def test_low_quality_gives_low_confidence(trainer):
    """Test poor quality gives low confidence even with samples."""
    confidence = trainer._calculate_confidence(
        sample_count=15,
        avg_quality=2.5,
        correctness_rate=0.5
    )

    assert confidence == "low"


def test_aggregate_feedback_by_pattern(trainer):
    """Test aggregating feedback by pattern and model."""
    # Requires test data in database
    performance_data = trainer._aggregate_feedback()

    assert isinstance(performance_data, dict)
    # Each pattern should have model stats
    for pattern, models in performance_data.items():
        assert 'count' in models
        assert 'avg_quality' in models
        assert 'correctness' in models


def test_retrain_only_updates_confident_patterns(trainer):
    """Test retraining only changes high/medium confidence patterns."""
    # Get initial weights
    initial_weights = trainer._get_current_weights()

    # Run retraining
    changes = trainer.retrain(dry_run=True)

    # Verify only confident patterns in changes
    for change in changes:
        assert change['confidence'] in ['high', 'medium']
        assert change['sample_count'] >= 5


def test_retrain_logs_run_metadata(trainer):
    """Test retraining logs metadata to database."""
    result = trainer.retrain(dry_run=False)

    assert 'run_id' in result
    assert 'timestamp' in result
    assert 'patterns_updated' in result
    assert 'total_changes' in result
```

### Step 2: Run test to verify it fails

```bash
pytest tests/test_feedback_trainer.py -v
```

**Expected:** FAIL with "ModuleNotFoundError"

### Step 3: Implement FeedbackTrainer

**Create:** `app/learning/feedback_trainer.py`

```python
"""Automated learning pipeline from user feedback."""
import logging
import uuid
from datetime import datetime
from typing import Dict, List, Any, Optional
from app.database.postgres import get_connection, get_cursor
from app.learning import QueryPatternAnalyzer

logger = logging.getLogger(__name__)


class FeedbackTrainer:
    """Retrains routing recommendations from user feedback.

    Uses confidence-based thresholds to only update routing when
    sufficient quality feedback has been collected.
    """

    # Confidence thresholds
    HIGH_CONFIDENCE_THRESHOLD = 10
    MEDIUM_CONFIDENCE_THRESHOLD = 5
    MIN_QUALITY_SCORE = 3.5
    MIN_CORRECTNESS_RATE = 0.7

    def __init__(self, db_url: Optional[str] = None):
        """Initialize trainer with database connection.

        Args:
            db_url: Optional database URL (for testing)
        """
        self.db_url = db_url
        self.analyzer = QueryPatternAnalyzer(db_path=None)  # Uses PostgreSQL

    def retrain(self, dry_run: bool = False) -> Dict[str, Any]:
        """Run retraining cycle.

        Args:
            dry_run: If True, preview changes without applying

        Returns:
            Retraining summary with changes made
        """
        run_id = str(uuid.uuid4())
        logger.info(f"Starting retraining run {run_id} (dry_run={dry_run})")

        # 1. Aggregate feedback by pattern + model
        performance_data = self._aggregate_feedback()

        changes = []

        # 2. Compute confidence and update routing
        for pattern, models_data in performance_data.items():
            for model, stats in models_data.items():
                confidence = self._calculate_confidence(
                    sample_count=stats['count'],
                    avg_quality=stats['avg_quality'],
                    correctness_rate=stats['correctness']
                )

                # 3. Only update if meets threshold
                if confidence in ['high', 'medium']:
                    change = {
                        'pattern': pattern,
                        'model': model,
                        'confidence': confidence,
                        'sample_count': stats['count'],
                        'avg_quality': stats['avg_quality'],
                        'correctness_rate': stats['correctness']
                    }

                    changes.append(change)

                    if not dry_run:
                        self._update_routing_weights(pattern, model, stats)
                        self._store_performance_history(
                            pattern, model, stats, confidence, run_id
                        )

        # 4. Log retraining run
        result = {
            'run_id': run_id,
            'timestamp': datetime.now().isoformat(),
            'patterns_updated': len(set(c['pattern'] for c in changes)),
            'total_changes': len(changes),
            'changes': changes,
            'dry_run': dry_run
        }

        if not dry_run:
            self._log_retraining_run(result)

        logger.info(f"Retraining complete: {result['total_changes']} changes")

        return result

    def _calculate_confidence(
        self,
        sample_count: int,
        avg_quality: float,
        correctness_rate: float
    ) -> str:
        """Calculate confidence level for pattern.

        Args:
            sample_count: Number of feedback samples
            avg_quality: Average quality score (1-5)
            correctness_rate: Correctness rate (0-1)

        Returns:
            Confidence level: 'high', 'medium', or 'low'
        """
        # Check quality thresholds
        if avg_quality < self.MIN_QUALITY_SCORE or correctness_rate < self.MIN_CORRECTNESS_RATE:
            return "low"

        # High confidence: lots of samples + excellent metrics
        if sample_count >= self.HIGH_CONFIDENCE_THRESHOLD and avg_quality >= 4.0 and correctness_rate >= 0.8:
            return "high"

        # Medium confidence: sufficient samples + good metrics
        if sample_count >= self.MEDIUM_CONFIDENCE_THRESHOLD:
            return "medium"

        # Low confidence: insufficient data
        return "low"

    def _aggregate_feedback(self) -> Dict[str, Dict[str, Dict[str, Any]]]:
        """Aggregate feedback by pattern and model.

        Returns:
            Nested dict: {pattern: {model: {stats}}}
        """
        with get_connection() as conn:
            cursor = get_cursor(conn, dict_cursor=True)

            cursor.execute("""
                SELECT
                    prompt_pattern,
                    selected_model,
                    COUNT(*) as sample_count,
                    AVG(quality_score) as avg_quality,
                    AVG(CASE WHEN is_correct THEN 1.0 ELSE 0.0 END) as correctness_rate,
                    AVG(complexity_score) as avg_complexity
                FROM response_feedback
                WHERE prompt_pattern IS NOT NULL
                  AND selected_model IS NOT NULL
                  AND timestamp > NOW() - INTERVAL '90 days'
                GROUP BY prompt_pattern, selected_model
                HAVING COUNT(*) >= 3
            """)

            rows = cursor.fetchall()

            # Organize by pattern -> model
            result = {}
            for row in rows:
                pattern = row['prompt_pattern']
                model = row['selected_model']

                if pattern not in result:
                    result[pattern] = {}

                result[pattern][model] = {
                    'count': row['sample_count'],
                    'avg_quality': float(row['avg_quality']),
                    'correctness': float(row['correctness_rate']),
                    'avg_complexity': float(row['avg_complexity']) if row['avg_complexity'] else 0.5
                }

            return result

    def _update_routing_weights(
        self,
        pattern: str,
        model: str,
        stats: Dict[str, Any]
    ):
        """Update routing weights for pattern-model pair.

        Args:
            pattern: Prompt pattern
            model: Model name
            stats: Performance statistics
        """
        # Update QueryPatternAnalyzer weights
        # This is application-specific logic
        logger.info(f"Updating weights for {pattern} -> {model}: quality={stats['avg_quality']:.2f}")

        # Implementation depends on how QueryPatternAnalyzer stores recommendations
        # For now, just log the update
        pass

    def _store_performance_history(
        self,
        pattern: str,
        model: str,
        stats: Dict[str, Any],
        confidence: str,
        run_id: str
    ):
        """Store performance metrics to history table.

        Args:
            pattern: Prompt pattern
            model: Model name
            stats: Performance statistics
            confidence: Confidence level
            run_id: Retraining run ID
        """
        with get_connection() as conn:
            cursor = get_cursor(conn, dict_cursor=False)

            # Extract provider from model name
            provider = model.split('/')[0] if '/' in model else 'unknown'

            cursor.execute("""
                INSERT INTO model_performance_history (
                    pattern, provider, model,
                    avg_quality_score, correctness_rate, sample_count, confidence_level,
                    avg_cost, total_cost, updated_at, retraining_run_id
                ) VALUES (
                    %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
                )
            """, (
                pattern,
                provider,
                model,
                stats['avg_quality'],
                stats['correctness'],
                stats['count'],
                confidence,
                None,  # avg_cost - compute if needed
                None,  # total_cost
                datetime.now(),
                run_id
            ))

    def _log_retraining_run(self, result: Dict[str, Any]):
        """Log retraining run metadata.

        Args:
            result: Retraining result summary
        """
        logger.info(f"Retraining run {result['run_id']}: {result['total_changes']} changes applied")

    def _get_current_weights(self) -> Dict[str, Any]:
        """Get current routing weights for comparison.

        Returns:
            Current weights
        """
        # Placeholder for getting current state
        return {}
```

### Step 4: Run test to verify it passes

```bash
pytest tests/test_feedback_trainer.py -v
```

**Expected:** PASS (7 tests)

### Step 5: Commit

```bash
git add app/learning/feedback_trainer.py tests/test_feedback_trainer.py
git commit -m "feat: add FeedbackTrainer learning pipeline

- Confidence-based retraining (high/medium/low thresholds)
- Aggregates feedback by pattern and model
- Only updates routing with sufficient samples
- Stores performance history for auditing
- Dry-run mode for testing
- Full test coverage

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 4: Docker Compose Setup

**Goal:** Create Docker Compose configuration for PostgreSQL and FastAPI

**Files:**
- Create: `docker-compose.yml`
- Create: `docker-compose.dev.yml`
- Create: `.env.example`
- Modify: `Dockerfile`

### Step 1: Create production Docker Compose

**Create:** `docker-compose.yml`

```yaml
version: '3.8'

services:
  postgres:
    image: postgres:15-alpine
    container_name: optimizer-db
    environment:
      POSTGRES_DB: optimizer
      POSTGRES_USER: optimizer_user
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./alembic/versions:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U optimizer_user"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - optimizer-network

  api:
    build: .
    container_name: optimizer-api
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      DATABASE_URL: postgresql://optimizer_user:${DB_PASSWORD}@postgres:5432/optimizer
      GOOGLE_API_KEY: ${GOOGLE_API_KEY}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
      OPENROUTER_API_KEY: ${OPENROUTER_API_KEY}
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
    ports:
      - "8000:8000"
    restart: unless-stopped
    networks:
      - optimizer-network
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000

  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: optimizer-pgadmin
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_EMAIL:-admin@optimizer.local}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_PASSWORD}
    ports:
      - "5050:80"
    depends_on:
      - postgres
    restart: unless-stopped
    networks:
      - optimizer-network

volumes:
  postgres_data:
    driver: local

networks:
  optimizer-network:
    driver: bridge
```

### Step 2: Create development Docker Compose

**Create:** `docker-compose.dev.yml`

```yaml
version: '3.8'

services:
  postgres:
    extends:
      file: docker-compose.yml
      service: postgres
    ports:
      - "5433:5432"  # Different port for dev

  api:
    extends:
      file: docker-compose.yml
      service: api
    volumes:
      - ./app:/app/app  # Mount for hot reload
      - ./tests:/app/tests
    environment:
      LOG_LEVEL: DEBUG
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

### Step 3: Update Dockerfile

**Modify:** `Dockerfile`

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    postgresql-client \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY app/ ./app/
COPY alembic/ ./alembic/
COPY alembic.ini .

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=40s \
  CMD curl -f http://localhost:8000/health || exit 1

EXPOSE 8000

# Default command (can be overridden by docker-compose)
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Step 4: Create environment template

**Create:** `.env.example`

```bash
# Database
DB_PASSWORD=your_secure_password_here

# API Keys
GOOGLE_API_KEY=your_google_api_key
ANTHROPIC_API_KEY=your_anthropic_key
OPENROUTER_API_KEY=your_openrouter_key

# Optional
PGADMIN_EMAIL=admin@optimizer.local
PGADMIN_PASSWORD=admin_password
LOG_LEVEL=INFO
```

### Step 5: Test Docker build

```bash
# Build image
docker-compose build

# Start services
docker-compose up -d

# Check services are running
docker-compose ps

# Check logs
docker-compose logs api

# Test health endpoint
curl http://localhost:8000/health

# Stop services
docker-compose down
```

**Expected:** Services start successfully, health endpoint returns 200

### Step 6: Commit

```bash
git add docker-compose.yml docker-compose.dev.yml Dockerfile .env.example
git commit -m "feat: add Docker Compose production setup

- PostgreSQL container with persistent volume
- FastAPI container with health checks
- pgAdmin for database management
- Development override for hot reload
- Network isolation for services
- Environment-based configuration

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 5: Admin Monitoring Endpoints

**Goal:** Add admin endpoints for monitoring learning progress

**Files:**
- Modify: `app/main.py`
- Create: `app/models/admin.py`
- Test: `tests/test_admin_endpoints.py`

### Step 1: Write tests for admin endpoints

**Create:** `tests/test_admin_endpoints.py`

```python
"""Tests for admin monitoring endpoints."""
import pytest
from fastapi.testclient import TestClient
from app.main import app


client = TestClient(app)


def test_feedback_summary_endpoint():
    """Test /admin/feedback/summary returns stats."""
    response = client.get("/admin/feedback/summary")

    assert response.status_code == 200
    data = response.json()

    assert "total_feedback" in data
    assert "avg_quality_score" in data
    assert "models" in data
    assert isinstance(data["models"], list)


def test_learning_status_endpoint():
    """Test /admin/learning/status returns status."""
    response = client.get("/admin/learning/status")

    assert response.status_code == 200
    data = response.json()

    assert "last_retraining_run" in data
    assert "confidence_distribution" in data
    assert "high" in data["confidence_distribution"]
    assert "medium" in data["confidence_distribution"]
    assert "low" in data["confidence_distribution"]


def test_retrain_dry_run_endpoint():
    """Test /admin/learning/retrain?dry_run=true."""
    response = client.post("/admin/learning/retrain?dry_run=true")

    assert response.status_code == 200
    data = response.json()

    assert "changes" in data
    assert "total_changes" in data
    assert data["dry_run"] is True


def test_retrain_actual_endpoint():
    """Test /admin/learning/retrain without dry_run."""
    response = client.post("/admin/learning/retrain?dry_run=false")

    assert response.status_code == 200
    data = response.json()

    assert "run_id" in data
    assert data["dry_run"] is False


def test_performance_trends_endpoint():
    """Test /admin/performance/trends."""
    response = client.get("/admin/performance/trends?pattern=code")

    assert response.status_code == 200
    data = response.json()

    assert "pattern" in data
    assert "trends" in data
    assert isinstance(data["trends"], list)
```

### Step 2: Run tests to verify they fail

```bash
pytest tests/test_admin_endpoints.py -v
```

**Expected:** FAIL with 404 Not Found

### Step 3: Create admin response models

**Create:** `app/models/admin.py`

```python
"""Pydantic models for admin endpoints."""
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
from datetime import datetime


class FeedbackSummary(BaseModel):
    """Summary of feedback statistics."""
    total_feedback: int
    avg_quality_score: float
    models: List[Dict[str, Any]]


class LearningStatus(BaseModel):
    """Status of learning pipeline."""
    last_retraining_run: Optional[str]
    next_scheduled_run: Optional[str]
    confidence_distribution: Dict[str, int]
    total_patterns: int


class RetrainingResult(BaseModel):
    """Result of retraining run."""
    run_id: str
    timestamp: str
    patterns_updated: int
    total_changes: int
    changes: List[Dict[str, Any]]
    dry_run: bool


class PerformanceTrends(BaseModel):
    """Performance trends for a pattern."""
    pattern: str
    trends: List[Dict[str, Any]]
```

### Step 4: Add admin endpoints to main.py

**Modify:** `app/main.py`

Add imports:
```python
from app.models.admin import (
    FeedbackSummary, LearningStatus,
    RetrainingResult, PerformanceTrends
)
from app.learning.feedback_trainer import FeedbackTrainer
```

Add endpoints:
```python
# Initialize trainer
feedback_trainer = FeedbackTrainer()


@app.get("/admin/feedback/summary", response_model=FeedbackSummary)
async def get_feedback_summary():
    """Get feedback statistics summary."""
    with get_connection() as conn:
        cursor = get_cursor(conn, dict_cursor=True)

        # Total feedback and avg quality
        cursor.execute("""
            SELECT
                COUNT(*) as total,
                AVG(quality_score) as avg_quality
            FROM response_feedback
        """)

        stats = cursor.fetchone()

        # Per-model stats
        cursor.execute("""
            SELECT
                selected_model,
                COUNT(*) as count,
                AVG(quality_score) as avg_quality,
                AVG(CASE WHEN is_correct THEN 1.0 ELSE 0.0 END) as correctness_rate
            FROM response_feedback
            WHERE selected_model IS NOT NULL
            GROUP BY selected_model
            ORDER BY count DESC
        """)

        models = cursor.fetchall()

        return FeedbackSummary(
            total_feedback=stats['total'],
            avg_quality_score=float(stats['avg_quality'] or 0),
            models=[dict(m) for m in models]
        )


@app.get("/admin/learning/status", response_model=LearningStatus)
async def get_learning_status():
    """Get learning pipeline status."""
    with get_connection() as conn:
        cursor = get_cursor(conn, dict_cursor=True)

        # Get last run from performance history
        cursor.execute("""
            SELECT
                retraining_run_id,
                MAX(updated_at) as last_run
            FROM model_performance_history
            GROUP BY retraining_run_id
            ORDER BY last_run DESC
            LIMIT 1
        """)

        last_run = cursor.fetchone()

        # Get confidence distribution
        cursor.execute("""
            SELECT
                confidence_level,
                COUNT(DISTINCT pattern) as count
            FROM model_performance_history
            WHERE retraining_run_id = (
                SELECT retraining_run_id
                FROM model_performance_history
                ORDER BY updated_at DESC
                LIMIT 1
            )
            GROUP BY confidence_level
        """)

        conf_dist = {row['confidence_level']: row['count'] for row in cursor.fetchall()}

        return LearningStatus(
            last_retraining_run=last_run['last_run'].isoformat() if last_run else None,
            next_scheduled_run=None,  # TODO: Add scheduler info
            confidence_distribution={
                'high': conf_dist.get('high', 0),
                'medium': conf_dist.get('medium', 0),
                'low': conf_dist.get('low', 0)
            },
            total_patterns=sum(conf_dist.values())
        )


@app.post("/admin/learning/retrain", response_model=RetrainingResult)
async def trigger_retraining(dry_run: bool = True):
    """Manually trigger retraining.

    Args:
        dry_run: If True, preview changes without applying

    Returns:
        Retraining result summary
    """
    result = feedback_trainer.retrain(dry_run=dry_run)

    return RetrainingResult(**result)


@app.get("/admin/performance/trends", response_model=PerformanceTrends)
async def get_performance_trends(pattern: str):
    """Get performance trends for a pattern.

    Args:
        pattern: Pattern to analyze (e.g., 'code', 'explanation')

    Returns:
        Performance trends over time
    """
    with get_connection() as conn:
        cursor = get_cursor(conn, dict_cursor=True)

        cursor.execute("""
            SELECT
                model,
                avg_quality_score,
                correctness_rate,
                sample_count,
                confidence_level,
                updated_at
            FROM model_performance_history
            WHERE pattern = %s
            ORDER BY updated_at DESC
            LIMIT 20
        """, (pattern,))

        trends = cursor.fetchall()

        return PerformanceTrends(
            pattern=pattern,
            trends=[dict(t) for t in trends]
        )
```

### Step 5: Run tests to verify they pass

```bash
pytest tests/test_admin_endpoints.py -v
```

**Expected:** PASS (5 tests)

### Step 6: Commit

```bash
git add app/models/admin.py app/main.py tests/test_admin_endpoints.py
git commit -m "feat: add admin monitoring endpoints

- GET /admin/feedback/summary - feedback statistics
- GET /admin/learning/status - pipeline status
- POST /admin/learning/retrain - manual retraining trigger
- GET /admin/performance/trends - pattern performance over time
- Full test coverage

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 6: SQLite to PostgreSQL Migration Script

**Goal:** Create script to migrate existing SQLite data to PostgreSQL

**Files:**
- Create: `scripts/migrate_to_postgres.sh`
- Create: `scripts/import_sqlite_data.py`

### Step 1: Create migration shell script

**Create:** `scripts/migrate_to_postgres.sh`

```bash
#!/bin/bash
set -e

echo "========================================="
echo "SQLite to PostgreSQL Migration"
echo "========================================="

# Check if SQLite database exists
if [ ! -f "optimizer.db" ]; then
    echo "ERROR: optimizer.db not found"
    exit 1
fi

# Backup SQLite database
echo "1. Backing up SQLite database..."
cp optimizer.db optimizer.db.backup
echo "   âœ“ Backup created: optimizer.db.backup"

# Export SQLite data
echo "2. Exporting SQLite data..."
sqlite3 optimizer.db .dump > sqlite_export.sql
echo "   âœ“ Data exported to sqlite_export.sql"

# Start PostgreSQL
echo "3. Starting PostgreSQL..."
docker-compose up -d postgres
echo "   âœ“ PostgreSQL container started"

# Wait for PostgreSQL to be ready
echo "4. Waiting for PostgreSQL..."
sleep 10

MAX_RETRIES=30
for i in $(seq 1 $MAX_RETRIES); do
    if docker exec optimizer-db pg_isready -U optimizer_user > /dev/null 2>&1; then
        echo "   âœ“ PostgreSQL is ready"
        break
    fi

    if [ $i -eq $MAX_RETRIES ]; then
        echo "   ERROR: PostgreSQL failed to start"
        exit 1
    fi

    echo "   Waiting... ($i/$MAX_RETRIES)"
    sleep 2
done

# Run Alembic migrations
echo "5. Running Alembic migrations..."
alembic upgrade head
echo "   âœ“ Database schema created"

# Import SQLite data
echo "6. Importing SQLite data to PostgreSQL..."
python scripts/import_sqlite_data.py
echo "   âœ“ Data imported successfully"

# Verify migration
echo "7. Verifying migration..."
python scripts/verify_migration.py
echo "   âœ“ Migration verified"

echo "========================================="
echo "Migration complete!"
echo "========================================="
echo ""
echo "Next steps:"
echo "1. Update .env with DATABASE_URL for PostgreSQL"
echo "2. Start all services: docker-compose up -d"
echo "3. Test API: curl http://localhost:8000/health"
```

Make executable:
```bash
chmod +x scripts/migrate_to_postgres.sh
```

### Step 2: Create Python import script

**Create:** `scripts/import_sqlite_data.py`

```python
"""Import SQLite data to PostgreSQL."""
import sqlite3
import psycopg2
from psycopg2.extras import execute_values
import os


def get_postgres_conn():
    """Get PostgreSQL connection."""
    db_password = os.getenv('DB_PASSWORD', 'password')
    return psycopg2.connect(
        f"postgresql://optimizer_user:{db_password}@localhost:5432/optimizer"
    )


def migrate_table(sqlite_conn, pg_conn, table_name, column_mapping=None):
    """Migrate a table from SQLite to PostgreSQL.

    Args:
        sqlite_conn: SQLite connection
        pg_conn: PostgreSQL connection
        table_name: Table to migrate
        column_mapping: Optional dict to rename columns
    """
    print(f"  Migrating {table_name}...")

    sqlite_cursor = sqlite_conn.cursor()
    pg_cursor = pg_conn.cursor()

    # Get all rows
    sqlite_cursor.execute(f"SELECT * FROM {table_name}")
    rows = sqlite_cursor.fetchall()

    if not rows:
        print(f"    No data in {table_name}")
        return

    # Get column names
    columns = [desc[0] for desc in sqlite_cursor.description]

    # Apply column mapping if provided
    if column_mapping:
        columns = [column_mapping.get(col, col) for col in columns]

    # Insert into PostgreSQL
    insert_sql = f"""
        INSERT INTO {table_name} ({', '.join(columns)})
        VALUES %s
        ON CONFLICT DO NOTHING
    """

    execute_values(pg_cursor, insert_sql, rows)
    pg_conn.commit()

    print(f"    âœ“ Migrated {len(rows)} rows")


def main():
    """Run migration."""
    print("Connecting to databases...")

    sqlite_conn = sqlite3.connect('optimizer.db')
    pg_conn = get_postgres_conn()

    print("Starting data migration...\n")

    # Migrate tables in dependency order
    tables = [
        'requests',
        'routing_metrics',
        'response_cache',
        'response_feedback'
    ]

    for table in tables:
        try:
            migrate_table(sqlite_conn, pg_conn, table)
        except Exception as e:
            print(f"    ERROR: {e}")
            # Continue with other tables

    print("\nData migration complete!")

    sqlite_conn.close()
    pg_conn.close()


if __name__ == '__main__':
    main()
```

### Step 3: Create verification script

**Create:** `scripts/verify_migration.py`

```python
"""Verify SQLite to PostgreSQL migration."""
import sqlite3
import psycopg2
import os


def verify_table_counts(sqlite_conn, pg_conn, table_name):
    """Verify row counts match.

    Args:
        sqlite_conn: SQLite connection
        pg_conn: PostgreSQL connection
        table_name: Table to verify

    Returns:
        True if counts match
    """
    sqlite_cursor = sqlite_conn.cursor()
    pg_cursor = pg_conn.cursor()

    # SQLite count
    sqlite_cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
    sqlite_count = sqlite_cursor.fetchone()[0]

    # PostgreSQL count
    pg_cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
    pg_count = pg_cursor.fetchone()[0]

    match = sqlite_count == pg_count
    status = "âœ“" if match else "âœ—"

    print(f"  {status} {table_name}: SQLite={sqlite_count}, PostgreSQL={pg_count}")

    return match


def main():
    """Run verification."""
    print("Verifying migration...\n")

    sqlite_conn = sqlite3.connect('optimizer.db')

    db_password = os.getenv('DB_PASSWORD', 'password')
    pg_conn = psycopg2.connect(
        f"postgresql://optimizer_user:{db_password}@localhost:5432/optimizer"
    )

    tables = ['requests', 'routing_metrics', 'response_cache']

    all_match = True
    for table in tables:
        try:
            if not verify_table_counts(sqlite_conn, pg_conn, table):
                all_match = False
        except Exception as e:
            print(f"  âœ— {table}: ERROR - {e}")
            all_match = False

    print()
    if all_match:
        print("âœ“ All table counts match!")
    else:
        print("âœ— Some tables have mismatched counts")
        return 1

    sqlite_conn.close()
    pg_conn.close()

    return 0


if __name__ == '__main__':
    exit(main())
```

### Step 4: Test migration script

```bash
# Run migration
./scripts/migrate_to_postgres.sh
```

**Expected:** Script completes with all checks passing

### Step 5: Commit

```bash
git add scripts/migrate_to_postgres.sh scripts/import_sqlite_data.py scripts/verify_migration.py
chmod +x scripts/migrate_to_postgres.sh
git commit -m "feat: add SQLite to PostgreSQL migration scripts

- Automated migration shell script
- Python data import with conflict handling
- Verification script for data integrity
- Backup creation before migration

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 7: Scheduled Retraining with APScheduler

**Goal:** Add weekly automated retraining scheduler

**Files:**
- Create: `app/scheduler.py`
- Modify: `app/main.py`
- Modify: `requirements.txt`
- Test: `tests/test_scheduler.py`

### Step 1: Add APScheduler dependency

**Modify:** `requirements.txt`

Add:
```
APScheduler>=3.10.4
```

Install:
```bash
pip install -r requirements.txt
```

### Step 2: Write scheduler tests

**Create:** `tests/test_scheduler.py`

```python
"""Tests for retraining scheduler."""
import pytest
from app.scheduler import RetrainingScheduler


def test_scheduler_initialization():
    """Test scheduler can be initialized."""
    scheduler = RetrainingScheduler()

    assert scheduler is not None
    assert not scheduler.is_running()


def test_scheduler_start_stop():
    """Test starting and stopping scheduler."""
    scheduler = RetrainingScheduler()

    scheduler.start()
    assert scheduler.is_running()

    scheduler.stop()
    assert not scheduler.is_running()


def test_scheduler_has_retraining_job():
    """Test scheduler has retraining job configured."""
    scheduler = RetrainingScheduler()
    scheduler.start()

    jobs = scheduler.get_jobs()

    assert len(jobs) > 0
    assert any('retrain' in job.id for job in jobs)

    scheduler.stop()


def test_manual_trigger_while_running():
    """Test manual retraining can be triggered while scheduler running."""
    scheduler = RetrainingScheduler()
    scheduler.start()

    # Should not raise error
    result = scheduler.trigger_immediate_retraining(dry_run=True)

    assert result is not None
    assert 'run_id' in result

    scheduler.stop()
```

### Step 3: Run tests to verify they fail

```bash
pytest tests/test_scheduler.py -v
```

**Expected:** FAIL with ModuleNotFoundError

### Step 4: Implement scheduler

**Create:** `app/scheduler.py`

```python
"""Scheduled retraining with APScheduler."""
import logging
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
from app.learning.feedback_trainer import FeedbackTrainer

logger = logging.getLogger(__name__)


class RetrainingScheduler:
    """Manages scheduled retraining jobs.

    Default schedule: Every Sunday at 2:00 AM
    """

    def __init__(self, cron_schedule: str = "0 2 * * 0"):
        """Initialize scheduler.

        Args:
            cron_schedule: Cron expression for retraining schedule
                          Default: "0 2 * * 0" (Sundays at 2 AM)
        """
        self.scheduler = BackgroundScheduler()
        self.trainer = FeedbackTrainer()
        self.cron_schedule = cron_schedule

        # Add retraining job
        self.scheduler.add_job(
            func=self._run_retraining,
            trigger=CronTrigger.from_crontab(cron_schedule),
            id='weekly_retraining',
            name='Weekly Feedback Retraining',
            replace_existing=True
        )

    def start(self):
        """Start scheduler."""
        if not self.scheduler.running:
            self.scheduler.start()
            logger.info(f"Retraining scheduler started: {self.cron_schedule}")

    def stop(self):
        """Stop scheduler."""
        if self.scheduler.running:
            self.scheduler.shutdown()
            logger.info("Retraining scheduler stopped")

    def is_running(self) -> bool:
        """Check if scheduler is running.

        Returns:
            True if running
        """
        return self.scheduler.running

    def get_jobs(self):
        """Get scheduled jobs.

        Returns:
            List of scheduled jobs
        """
        return self.scheduler.get_jobs()

    def trigger_immediate_retraining(self, dry_run: bool = False):
        """Manually trigger retraining immediately.

        Args:
            dry_run: If True, preview changes without applying

        Returns:
            Retraining result
        """
        logger.info(f"Manual retraining triggered (dry_run={dry_run})")
        return self._run_retraining(dry_run=dry_run)

    def _run_retraining(self, dry_run: bool = False):
        """Run retraining job.

        Args:
            dry_run: If True, preview changes only

        Returns:
            Retraining result
        """
        try:
            logger.info("Starting scheduled retraining...")
            result = self.trainer.retrain(dry_run=dry_run)

            logger.info(
                f"Retraining complete: {result['total_changes']} changes, "
                f"{result['patterns_updated']} patterns updated"
            )

            return result

        except Exception as e:
            logger.error(f"Retraining failed: {e}", exc_info=True)
            raise
```

### Step 5: Integrate scheduler with FastAPI

**Modify:** `app/main.py`

Add import:
```python
from app.scheduler import RetrainingScheduler
import os
```

Add after app initialization:
```python
# Initialize retraining scheduler
scheduler = None

if os.getenv('ENABLE_SCHEDULER', 'true').lower() == 'true':
    scheduler = RetrainingScheduler()


@app.on_event("startup")
async def startup_event():
    """Start scheduler on app startup."""
    if scheduler:
        scheduler.start()
        logger.info("Retraining scheduler started")


@app.on_event("shutdown")
async def shutdown_event():
    """Stop scheduler on app shutdown."""
    if scheduler:
        scheduler.stop()
        logger.info("Retraining scheduler stopped")
```

Update retrain endpoint to use scheduler:
```python
@app.post("/admin/learning/retrain", response_model=RetrainingResult)
async def trigger_retraining(dry_run: bool = True):
    """Manually trigger retraining.

    Args:
        dry_run: If True, preview changes without applying

    Returns:
        Retraining result summary
    """
    if scheduler:
        result = scheduler.trigger_immediate_retraining(dry_run=dry_run)
    else:
        result = feedback_trainer.retrain(dry_run=dry_run)

    return RetrainingResult(**result)
```

### Step 6: Run tests to verify they pass

```bash
pytest tests/test_scheduler.py -v
```

**Expected:** PASS (4 tests)

### Step 7: Commit

```bash
git add app/scheduler.py app/main.py requirements.txt tests/test_scheduler.py
git commit -m "feat: add scheduled retraining with APScheduler

- Weekly retraining every Sunday at 2 AM
- Manual trigger via admin endpoint
- Configurable via ENABLE_SCHEDULER env var
- Graceful startup/shutdown with FastAPI
- Full test coverage

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 8: Integration Testing & Documentation

**Goal:** End-to-end integration tests and deployment documentation

**Files:**
- Create: `tests/test_integration_feedback_loop.py`
- Create: `docs/DEPLOYMENT.md`
- Create: `scripts/startup.sh`
- Create: `scripts/shutdown.sh`

### Step 1: Write integration test

**Create:** `tests/test_integration_feedback_loop.py`

```python
"""End-to-end integration tests for feedback loop."""
import pytest
import time
from fastapi.testclient import TestClient
from app.main import app


client = TestClient(app)


def test_complete_feedback_loop_flow():
    """Test complete flow: request -> feedback -> retrain -> improved routing.

    This tests the entire system end-to-end:
    1. Submit a prompt and get routing decision
    2. Provide quality feedback
    3. Trigger retraining
    4. Verify routing recommendations updated
    """
    # Step 1: Submit request with auto_route
    response = client.post("/complete", json={
        "prompt": "Debug Python code with print statements",
        "auto_route": True,
        "max_tokens": 100
    })

    assert response.status_code == 200
    completion = response.json()

    request_id = completion.get("request_id")
    assert request_id is not None

    # Step 2: Submit positive feedback
    feedback_response = client.post("/feedback", json={
        "request_id": request_id,
        "quality_score": 5,
        "is_correct": True,
        "comment": "Excellent code debugging explanation"
    })

    assert feedback_response.status_code == 200
    assert feedback_response.json()["status"] == "recorded"

    # Step 3: Check feedback summary
    summary = client.get("/admin/feedback/summary")
    assert summary.status_code == 200
    assert summary.json()["total_feedback"] > 0

    # Step 4: Run retraining (dry run first)
    dry_run = client.post("/admin/learning/retrain?dry_run=true")
    assert dry_run.status_code == 200

    dry_result = dry_run.json()
    assert "changes" in dry_result
    assert dry_result["dry_run"] is True

    # Step 5: Check learning status
    status = client.get("/admin/learning/status")
    assert status.status_code == 200

    status_data = status.json()
    assert "confidence_distribution" in status_data


def test_feedback_improves_routing_over_time():
    """Test that repeated good feedback improves routing confidence.

    Simulates multiple users providing positive feedback for
    a specific pattern-model combination.
    """
    pattern_prompt = "Explain asyncio in Python"

    # Submit 10 requests with feedback
    for i in range(10):
        # Get routing decision
        response = client.post("/complete", json={
            "prompt": pattern_prompt,
            "auto_route": True,
            "max_tokens": 50
        })

        request_id = response.json()["request_id"]

        # Provide positive feedback
        client.post("/feedback", json={
            "request_id": request_id,
            "quality_score": 5,
            "is_correct": True
        })

        time.sleep(0.1)  # Small delay

    # Run retraining
    retrain_result = client.post("/admin/learning/retrain?dry_run=false")
    assert retrain_result.status_code == 200

    result = retrain_result.json()

    # Should have high confidence updates now
    high_confidence = [
        c for c in result["changes"]
        if c["confidence"] == "high"
    ]

    # May not have high confidence yet with only 10 samples,
    # but should have at least medium
    assert result["total_changes"] > 0


def test_low_quality_feedback_prevents_routing_change():
    """Test that poor quality feedback doesn't change routing.

    Ensures low-quality responses don't pollute the learning.
    """
    # Submit request
    response = client.post("/complete", json={
        "prompt": "Test low quality pattern",
        "auto_route": True,
        "max_tokens": 50
    })

    request_id = response.json()["request_id"]

    # Provide negative feedback
    for _ in range(5):
        client.post("/feedback", json={
            "request_id": request_id,
            "quality_score": 1,
            "is_correct": False,
            "comment": "Wrong answer"
        })

    # Run retraining (dry run)
    retrain_result = client.post("/admin/learning/retrain?dry_run=true")
    result = retrain_result.json()

    # Should NOT update routing with poor quality
    for change in result["changes"]:
        assert change["avg_quality"] >= 3.5  # Min quality threshold
```

### Step 2: Create deployment documentation

**Create:** `docs/DEPLOYMENT.md`

```markdown
# Deployment Guide

## Prerequisites

- Docker 20.10+
- Docker Compose 2.0+
- At least one LLM API key (Google, Anthropic, or OpenRouter)

## First-Time Setup

### 1. Clone and Configure

```bash
cd ai-cost-optimizer

# Create environment file
cp .env.example .env

# Edit with your API keys
nano .env
```

Required in `.env`:
```bash
DB_PASSWORD=your_secure_database_password
GOOGLE_API_KEY=your_key  # At least one required
ANTHROPIC_API_KEY=your_key
OPENROUTER_API_KEY=your_key
```

### 2. Migrate from SQLite (if upgrading)

If you have existing SQLite data:

```bash
./scripts/migrate_to_postgres.sh
```

Otherwise, start fresh:

```bash
docker-compose up -d postgres
sleep 10
alembic upgrade head
```

### 3. Start All Services

```bash
docker-compose up -d
```

Verify services:
```bash
docker-compose ps
```

Should show:
- `optimizer-db` (healthy)
- `optimizer-api` (healthy)
- `optimizer-pgadmin` (healthy)

### 4. Verify Deployment

```bash
# Health check
curl http://localhost:8000/health

# Test completion
curl -X POST http://localhost:8000/complete \
  -H "Content-Type: application/json" \
  -d '{"prompt": "What is AI?", "auto_route": true}'
```

## Daily Operations

### Starting Services

```bash
docker-compose up -d
```

### Stopping Services

```bash
docker-compose down
```

### Viewing Logs

```bash
# All services
docker-compose logs -f

# API only
docker-compose logs -f api

# PostgreSQL only
docker-compose logs -f postgres
```

### Database Management

Access pgAdmin at http://localhost:5050

Login:
- Email: From `.env` (default: admin@optimizer.local)
- Password: From `.env`

Connect to database:
- Host: postgres
- Port: 5432
- Database: optimizer
- Username: optimizer_user
- Password: From `.env`

## Monitoring

### Feedback Summary

```bash
curl http://localhost:8000/admin/feedback/summary | jq
```

### Learning Status

```bash
curl http://localhost:8000/admin/learning/status | jq
```

### Manual Retraining

Dry run (preview changes):
```bash
curl -X POST http://localhost:8000/admin/learning/retrain?dry_run=true | jq
```

Actual retraining:
```bash
curl -X POST http://localhost:8000/admin/learning/retrain?dry_run=false | jq
```

## Backup & Recovery

### Manual Backup

```bash
# Backup database
docker exec optimizer-db pg_dump -U optimizer_user optimizer \
  | gzip > backup_$(date +%Y%m%d).sql.gz
```

### Restore from Backup

```bash
# Stop API
docker-compose stop api

# Restore
gunzip -c backup_20251108.sql.gz \
  | docker exec -i optimizer-db psql -U optimizer_user optimizer

# Restart
docker-compose up -d
```

### Automated Backups

Add to crontab:
```bash
0 3 * * * /path/to/ai-cost-optimizer/scripts/backup_db.sh
```

## Troubleshooting

### API Won't Start

Check logs:
```bash
docker-compose logs api
```

Common issues:
- Database not ready â†’ Wait 30s and restart
- Missing env vars â†’ Check `.env` file
- Port conflict â†’ Change port in `docker-compose.yml`

### Database Connection Failed

```bash
# Check PostgreSQL is running
docker-compose ps postgres

# Test connection
docker exec optimizer-db pg_isready -U optimizer_user
```

### Retraining Not Working

```bash
# Check scheduler status
curl http://localhost:8000/admin/learning/status

# Check if enough feedback
curl http://localhost:8000/admin/feedback/summary
```

Need 5+ samples per pattern for medium confidence.

## Production Considerations

### Security

- Change default passwords in `.env`
- Use secrets management (not `.env` file)
- Enable SSL/TLS with reverse proxy
- Restrict pgAdmin access

### Scaling

- Use PostgreSQL managed service (AWS RDS, GCP Cloud SQL)
- Run multiple API containers with load balancer
- Use Redis for distributed caching
- Monitor with Prometheus + Grafana

### Performance

- Add database indexes for slow queries
- Tune PostgreSQL configuration
- Monitor API response times
- Set up alerts for errors

## Next Steps

After deployment:

1. **Week 1**: Collect feedback, monitor quality
2. **Week 2**: Review first retraining results
3. **Week 3**: Enable automated weekly retraining
4. **Week 4+**: Optimize based on metrics

See [design document](plans/2025-11-08-production-feedback-loop-design.md) for detailed roadmap.
```

### Step 3: Create startup script

**Create:** `scripts/startup.sh`

```bash
#!/bin/bash
set -e

echo "Starting AI Cost Optimizer..."

# Check .env exists
if [ ! -f ".env" ]; then
    echo "ERROR: .env file not found"
    echo "Run: cp .env.example .env"
    exit 1
fi

# Start services
docker-compose up -d

# Wait for health checks
echo "Waiting for services to be healthy..."
sleep 15

# Check health
if curl -f http://localhost:8000/health > /dev/null 2>&1; then
    echo "âœ“ API is healthy"
else
    echo "âœ— API failed to start"
    docker-compose logs api
    exit 1
fi

echo ""
echo "========================================="
echo "AI Cost Optimizer is running!"
echo "========================================="
echo ""
echo "API:      http://localhost:8000"
echo "Docs:     http://localhost:8000/docs"
echo "pgAdmin:  http://localhost:5050"
echo ""
echo "Logs:     docker-compose logs -f"
echo "Stop:     docker-compose down"
echo "========================================="
```

Make executable:
```bash
chmod +x scripts/startup.sh
```

### Step 4: Create shutdown script

**Create:** `scripts/shutdown.sh`

```bash
#!/bin/bash

echo "Stopping AI Cost Optimizer..."

docker-compose down

echo "âœ“ All services stopped"
```

Make executable:
```bash
chmod +x scripts/shutdown.sh
```

### Step 5: Run integration tests

```bash
pytest tests/test_integration_feedback_loop.py -v
```

**Expected:** PASS (3 tests)

### Step 6: Commit

```bash
git add tests/test_integration_feedback_loop.py docs/DEPLOYMENT.md scripts/startup.sh scripts/shutdown.sh
chmod +x scripts/startup.sh scripts/shutdown.sh
git commit -m "feat: add integration tests and deployment docs

- End-to-end feedback loop integration tests
- Complete deployment documentation
- Startup/shutdown convenience scripts
- Production checklist and troubleshooting guide

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Execution Handoff

Plan complete and saved to `docs/plans/2025-11-08-production-feedback-loop-implementation.md`.

**Two execution options:**

**1. Subagent-Driven (this session)**
- I dispatch fresh subagent per task
- Code review between tasks
- Fast iteration with quality gates

**2. Parallel Session (separate)**
- Open new session with executing-plans
- Batch execution with checkpoints
- Independent progress

**Which approach would you prefer?**
